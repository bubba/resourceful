\chapter{Background}\label{chapter:background}

\section{Separation logic}\label{sec:separationologic}
Much of the original inspiration for this work came from separation
logic, and how it can be used to formalise properties of concurrent
programs. Separation logic~\cite{ohearn2019,reynolds2002} is an
extension to Hoare logic, a system for formalising properties about
imperative programs, and has been used to prove properties about
resource access in concurrent programs~\cite{ohearn2007}. It is
formalised through \emph{specifications}, which consist of some code
that alongside a \emph{precondition} a \emph{postcondition}.
\[
  {\color{gray} \{\textit{precondition}\}} \
  \textit{code} \
  {\color{gray} \{\textit{postcondition}\}}
\]
The preconditions and postconditions make assertions about what points
to what inside the \textit{heap}. For example, in the example that
O'Hearn gives, a cyclic list is constructed from two pointers.
\begin{gather*}
  {\color{gray} \{ x \mapsto 0 * y \mapsto 0 \} }\\
  [x] = y; \\
  [y] = x; \\
  {\color{gray} \{ x \mapsto y * y \mapsto x \} }
\end{gather*}
The code sets the value of $x$ is set to the location of $y$, and
vice-versa. Separation logic extends Hoare logic with the notion of
splitting the heap. In the example above, instead of asserting
$x \mapsto y \wedge y \mapsto x$, the separation conjunction $*$ is used, which says
``$x$ points to $y$ \textit{and separately} $y$ points to $x$''. The
heap must be able to split into disjoint subheaps, such that
$x \mapsto y$ holds in one and $y \mapsto x$ in the other. This means that
$x$ and $y$ cannot point to the same location in the heap.
These preconditions and postconditions can be derived from so-called
small axioms, which for an imperative language might contain an axiom
for storing to the heap:
\[ {\color{gray} \{x \mapsto -\}} [x] = v {\color{gray} \{x \mapsto v\}} \]
The \textit{frame rule} allows a specification to be extended by
adding extra predicates that do not mention any variables used by the
code.
\[
  \infer{{\color{gray} \{p\}} \ \emph{code} \ {\color{gray} \{q\}}}
  {{\color{gray} \{p * r\}} \ \emph{code} \ {\color{gray} \{ q * r \}}} \
  \text{\parbox{2in}{where no variable
    occurring free in r is modified by \emph{code}}}
\]
This rule ends up being quite significant, as its the reason why
separation logic allows for programs to be reasoned about locally ---
the predicates involving the heap are shrunk so that they only contain
variables that the code modifies.

There is also the \emph{concurrency rule} for reasoning about
concurrency, which gives the precondition and postcondition needed for
two programs that are run in parallel.
\[
  \infer{ {\color{gray} \{p_1\}} \ \emph{code}_1 \ {\color{gray} \{q_1\}}
    \\
    {\color{gray} \{p_2\}} \ \emph{code}_2 \ {\color{gray} \{q_2\}}}
  { {\color{gray} \{p_1 * p_2\}} \ \emph{code}_1 \ || \ \emph{code}_2 \
    {\color{gray} \{q_1 * q_2\}}}
\]
We will see later on in Chapter~\ref{cha:evaluation} how the system we
end up developing parallels with this. 

Krishnaswami built upon separation logic with a higher-order ML like
language\cite{krishnaswami2006} with types. It utilised monadic
binding for sequencing, allowing imperative computation within an
otherwise pure language. Unlike our system, this included separation
logic predicates directly within the language. We will not work with
predicates, and instead push all our guarantees into the type level.

\section{Monads categorically}
As the language we will develop in Chapter~\ref{chapter:system} is
based around monads, it is a good idea to take a brief look at what
exactly they are, and dip our toes into the cool waters of category
theory. There is an infamous definition\footnote{\textsc{James Iry} A
  Brief, Incomplete, and Mostly Wrong History of Programming
  Languages} of a monad that goes along the lines of
\begin{quote}
  \textsl{A monad is just a monoid in the category of endofunctors,
    what's the problem?}
\end{quote}
Let's break this down. A \textit{category} is a collection of objects
(like a set), and morphisms (like functions between sets). In the same
way, Haskell has \textsf{Hask} --- the category of types. Its objects
are types such as \mintinline{haskell}{Bool} and
\mintinline{haskell}{Int}, and its morphisms are functions taking a
type and returning another.
\begin{minted}{haskell}
Bool, Int :: * -- Bool and Int are types
True :: Bool
42 :: Int
f : Bool -> Int -- a morphism between Bool and Int
\end{minted}
A \textit{functor} is a mapping between categories that
preserves the structure. That is, a functor maps objects and morphisms
from one category to objects and morphisms in another category.
In the same way, a \mintinline{haskell}{Functor} in Haskell maps a
type to another type, and its functions to other functions with
\mintinline{haskell}{fmap}. 
\begin{minted}{haskell}
Maybe :: * -> *
Maybe Bool :: * -- Maps Bool to Maybe Bool
fmap :: (a -> b) -> (f a -> f b) -- Maps (a -> b) to (f a -> f b)
\end{minted}
Functors can have mappings to other functors, and these are known as
\emph{natural transformations}.
An \textit{endofunctor} is a \textit{functor} from one category to the
same category. That is, it maps objects and morphisms from one
category, to objects and morphisms in the same category. But what does
that mean in Haskell? Well, since \mintinline{haskell}{Functor} maps
types to types, we are mapping from \textsf{Hask} to \textsf{Hask} ---
all \mintinline{haskell}{Functor}s in Haskell are endofunctors! They
exist as type constructors with kind \mintinline{haskell}{* -> *}.

A \textit{monad} is then defined as:
\begin{enumerate}
\item An endofunctor $T : X \rightarrow X$
\item With a natural transformation to flatten two monads together

${\mu : T(T(X)) \rightarrow T(X)}$
\item And another natural transformation $\eta : X \rightarrow T(X)$
\item That satisfy some properties (the monad laws)
\end{enumerate}

Or in Haskell,
\begin{minted}{haskell}
class Functor m => Monad m a where
  join :: m (m a) -> m a
  return :: a -> m a
\end{minted}
However as you might be aware, the actual definition in Haskell uses
bind \mintinline{haskell}{>>=}, not the mathematical definition of
\mintinline{haskell}{join}. It can be implemented in terms of
\mintinline{haskell}{>>=} instead, and vice-versa using the fact that
a monad is also a functor.
\begin{minted}{haskell}
(>>=) :: m a -> (a -> m b) -> m b
join m = m >>= \x -> x
m >>= f = join (fmap f m)
\end{minted}

The \textit{monoid in the category of endofunctors} bit then comes
about because a monoid is defined as:
\begin{enumerate}
\item A set $X$
\item With an associative binary operation $\circ : X \times X \rightarrow X$
\item And an identity element $e : X$
\item That satisfy some properties (the monoid laws)
\end{enumerate}

A monoid is the same deal as a monad, except functor composition is
replaced with the Cartesian product and the unit function is replaced
with the identity element.  And if monoids allow chaining together
elements, then monads in programming allow sequencing together
computation. In our system we will be using the more pragmatic Haskell
definition of a monad that uses bind rather than join. But the
fundamental idea of sequencing remains the same.

\section{Type-level programming within GHC}

\setminted[haskell]{breaklines}

Before creating the system in Chapter~\ref{chapter:system}, we
originally attempted to embed a resourceful system within GHC's type
system. Whilst the original Haskell 2010 language specification was
based on System F, there has been much
work~\cite{eisenberg2016}\cite{weirich2017} carried out to add
dependent types to GHC via a plethora of language extensions. Namely,
\texttt{TypeFamilies} and \texttt{GADT} provide a lot of the power
needed to carry out type-level programming, and we leveraged this
expressiveness to emulate a resource-tracking version of the
\mintinline{haskell}{IO} monad, dubbed \mintinline{haskell}{SubIO}. It
carried around a heap with a phantom type parameter:
\begin{minted}{haskell}
newtype SubIO (a :: [j]) b = SubIO (IO b)
  deriving (Functor, Applicative, Monad)
\end{minted}

We then defined a subclass of \mintinline{haskell}{Monad} that
extended it with a sequencing operator and concurrent operator, both
which merged the heap phantom type parameters together.
\begin{minted}{haskell}
class Monad (m j) => HeapMonad (m :: [x] -> * -> *) j where
  (>>>=) :: m j a -> (a -> m k b) -> m (j ** k) b
  (|||) :: m j a -> m k b -> m (j ** k) b
  (|||) x f = x >>>= const f
\end{minted}
The \texttt{**} operator was a type family that merged
together two heaps, and threw a type error if they overlapped.
\begin{minted}{haskell}
type family Overlap a b :: Bool where
  Overlap '[] b = 'False
  Overlap (a ': as) b = If (MemberP a b) 'True (Overlap as b)

type family a ** b :: [c] where
  (a ** b) = If (Overlap a b)
                (TypeError ('Text "Heaps overlap!"))
                (a :++ b)
\end{minted}
An instance for \mintinline{haskell}{SubIO} was given, and with it
some file operations from \mintinline{haskell}{System.IO}, transformed to accept
the file argument as a type argument, so it could be tracked in the heap.
\begin{minted}{haskell}
instance HeapMonad SubIO j where
  (SubIO x) >>>= f = SubIO (x >>= \z -> let (SubIO y) = f z in y)
  (SubIO x) ||| (SubIO y) = SubIO (forkIO (x >> pure ()) >> y)

readFile :: forall filePath. KnownSymbol filePath => SubIO '[filePath] String
readFile = fileOp Prelude.readFile

appendFile :: forall filePath. KnownSymbol filePath => String -> SubIO '[filePath] ()
appendFile x = fileOp (`Prelude.appendFile` x)

writeFile :: forall filePath. KnownSymbol filePath => String -> SubIO '[filePath] ()
writeFile x = fileOp (`Prelude.writeFile` x)

fileOp :: forall filePath a. KnownSymbol filePath => (FilePath -> IO a) -> SubIO '[filePath] a
fileOp f = let fp = symbolVal (Proxy :: Proxy filePath)
             in SubIO (f fp)
\end{minted}
All together, this meant that functions could be written with the
familiar do syntax, and ran concurrently with
\texttt{|||} --- but only if their heaps did not overlap.
\begin{minted}{haskell}
someIO = do
  writeFile @"foo.txt" "hello"
  s <- readFile
  return s

someMoreIO x = do
  appendFile @"bar.txt" x
  readFile >>= SubIO . putStrLn

concExample = runSubIO $ someIO ||| someMoreIO "blah"
--- this *should* type error
-- badConcExample = runSubIO $ someIO ||| someIO
\end{minted}
However we soon found out that overlapping instances don't always
throw a type error, as the phantom type parameter keeping track of the
heap was evaluated \textbf{lazily}. There was a workaround that
involved extracting the heap down to the term level and forcing
evaluation via \mintinline{haskell}{Data.Proxy} and
\mintinline{haskell}{seq}, but we think there are most likely better
ways to embed this within GHC's type system.

\section{Hindley-Damas-Milner}

One of our aims when developing this resource-tracking monadic type
system is to see how well it would integrate into existing functional
programming languages, specifically those of the ML family. So for
this reason we will basing our system off of the Hindley-Damas-Milner
(HDM) type system~\cite{damas1982}. Originally designed to formalise
the type system of ML, it is one of the first formalisations of a
polymorphic type system. It was heavily influential at the time and
still continues to be so today, paving the way for newer type systems
such as System F. 

\subsection{Syntax}

Before we can talk about a type system, we need to talk about the
language that it operates on. HDM uses an applicative language: A
language in which you can apply abstractions.
\def\defaultHypSeparation{\hskip .05in}
\newcommand{\letin}[2]{\mathsf{let} \ #1 \ \mathsf{in} \ #2}
\begin{grammar}

  <expression $e$> ::= $x$ | $\lambda x . e$ | $e \ e'$ | $\letin{x=e'}{e}$ % | $\square$

  <type $\tau$> ::= $\square$ | $\alpha$ | $\tau' \rightarrow \tau$
  
  <type scheme $\sigma$> ::= $\tau$ | $\forall \alpha . \sigma$

  <context $\Gamma$> ::= $\centerdot$ | $\Gamma, x : \sigma$

\end{grammar}
Expressions are an extension of the venerable lambda calculus, with the
addition of a new let expression that binds an expression to a
variable. As we will see later, this is notably different from
abstraction as it provides the gateway to polymorphism.

Types are either unit types, type variables or function types. However
expressions are not assigned types directly, instead they are given
type schemes which quantify over type variables. The distinction
between the two is necessary so that quantifiers can only
appear at the top level. 

A context is a linked list for looking up the type schemes of
variables. Whenever we need to work out the type of a variable
expression like $x$, we traverse the context to find its type scheme.

There is also a notion of \emph{free type variables}, which are type
variables \emph{inside a type scheme} which have not been bound
(quantified over).
\begin{align*}
  \ftv(\forall \alpha . \tau) &= \ftv(\tau) \setminus \{\alpha\} \\
  \ftv(\alpha) &= \{ \alpha \} \\
  \ftv(\tau \rightarrow \tau') &= \ftv(\tau') \cup \ftv(\tau)
\end{align*}

\subsection{Static semantics}

A type system describes how we assign \emph{valid} types to our
expressions. Most of the time, it's through a typing relation like this
\[ \Gamma \vdash e : \tau \]
You can read this as ``\textit{In the context
  $\Gamma$, $e$ has the type $\tau$}''. This is a relation between a context,
an expression and a type, in the same way that $a \leq b$ is a relation
between two numbers. With relations we can form \textit{judgements},
such as ${1 \leq 2}$, or
${\Gamma \vdash \lambda x . (y \ x) : \alpha \rightarrow \tau}$.  These are just statements that we can
make --- they might be true, they might be false. Is $42 \leq 19$? Does
${\Gamma \vdash z : \beta}$? I don't know, you tell me. We need something else to be
able to tell whether or not they make sense: whether or not they are
\textit{well typed}. So there are \emph{typing rules} that allow us to
prove that these typing judgements are indeed well typed, and that the
expressions can have the type that they claim to have.
\begin{mathpar}
  \inferrule*[Right=Var]{x : \sigma \in \Gamma \\ \sigma > \tau}{\Gamma \vdash x : \tau} \and
  \inferrule*[Right=App]{\Gamma \vdash e : \tau' \rightarrow \tau \\ \Gamma \vdash e' : \tau'}{\Gamma \vdash e \ e' : \tau} \\
  \inferrule*[Right=Abs]{\Gamma,x:\tau' \vdash e : \tau}{\Gamma \vdash \lambda x . e : \tau' \rightarrow \tau} \and
  \inferrule*[Right=Let]{\Gamma \vdash e' : \tau' \\ \Gamma,x : \bar{\Gamma}(\tau') \vdash e : \tau}
  {\Gamma \vdash \mathsf{let} \ x = e' \ \mathsf{in} \ e : \tau}
\end{mathpar}
These rules consist of some \emph{premises} above a line, and a
\emph{conclusion} below it. Premises and conclusions are just other
judgements, that we have been able to prove.
The gist of these rules is that if you have proof of all the premises
above, then you can infer the conclusion at the bottom. So for example, you
can read \textsc{App} as \textsl{``If $e$ has the function type $\tau' \rightarrow \tau$
  in $\Gamma$ and $e'$ has the type $\tau'$ also in $\Gamma$, then $e'$ applied to
  $e$ has the type $\tau$ in $\Gamma$ too.''}.

\textsc{App} is one of the four typing rules in the syntax-directed
HDM type system, and tells us what happens to the types when we apply
an argument to a function. Let's take a look at what the others mean.
\textsc{Abs} relates to functions, sometimes called abstractions. It
says if $e$ has the type $\tau$ in the context $\Gamma$, \textit{extended}
with $x$ having the type scheme $\tau'$, then $\lambda x.e$ has the type ${\tau' \rightarrow
\tau}$ in $\Gamma$. In other words, if $e$ can have type $\tau$ provided it has
access to $x : \tau'$, then we can make a lambda out of it.

If we have a variable expression $x$, then \textsc{Var} tells us how
we can get the type for it. First, we need to make sure $x$ exists in
the context $\Gamma$. It will have some type scheme $\sigma$, but we can't
directly assign that to an expression --- the typing relation assigns
types to terms, not type schemes. Instead, we need to instantiate it
to a type with $\sigma > \tau$. This instantiation relation says that if we
have a type scheme $\forall \alpha_1\ldots\alpha_n . \tau'$, then there exists a mapping of
type variables to types
${\alpha_1\mapsto\tau_1,\ldots,\alpha_n\mapsto\tau_n}$ which we can apply to those bound type variables
in $\tau'$ to give us $\tau$.

As it turns out, these mappings from type
variables to types are really common, and they are called
\emph{substitutions}.  The whole rule then means, if we can look up a
type scheme $\sigma$ for $x$, and then instantiate to some type
$\tau$, then we can infer that $x$ has the type $\tau$ in $\Gamma$.

The \textsc{Let} rule introduces polymorphism to the language. That
is, it allows one expression to be used for multiple types. Consider
the identity function
\[ \lambda x . x \]
What type should it have? It could be used both as a
$\mathsf{Int} \rightarrow \mathsf{Int}$. But the reality is that we don't care
about the underlying type, and want it to work on \emph{anything}.
\[ \Gamma \vdash \lambda x . x : \alpha \rightarrow \alpha \] Now with type variables instead, we have
nicely abstracted over the concrete types. Assuming we have a context
like $\Gamma = \centerdot , a : \mathsf{Int}$, we might try to use it multiple
times, each with different types, by passing it in as an argument to
abstraction.
\[ \Gamma \vdash (\lambda y . (y \ y) \ (y \ a)) \ (\lambda x . x) : \mathsf{Int} \]
But this will not work, and it is in fact ill-typed. Our identity
function will get passed around as ${y : \alpha \rightarrow \alpha}$ through \textsc{Abs}. When
either $(y \ y)$ or $(y \ a)$ try to look up $y$ in \textsc{Var}, they
will find that they can't instantiate it to the type they want,
because $\alpha$ is not quantified over!

Instead, we need to pass around the identity function as ${y : \forall \alpha . \alpha
\rightarrow \alpha}$. With a let expression, we can instead say
\[ \Gamma \vdash \letin{y = \lambda x . x}{(y \ y) \ (y \ a)} : \mathsf{Int} \]
If we look at the first premise for \textsc{Let}, we can show that
${\Gamma \vdash \lambda x . x : \alpha \rightarrow \alpha}$. In the second premise however, we put it into
the context as $y : \overline{\Gamma}(\alpha \rightarrow \alpha)$. $\overline{\Gamma}(\alpha \rightarrow \alpha)$ is the
close function, which is defined as
\[ \overline{\Gamma}(\tau) = \forall \alpha_1 \ldots \alpha_n . \tau \
\textsf{where} \ \{ \alpha_1, \ldots, \alpha_n \} = \ftv(\tau) \setminus \ftv(\Gamma) \]
This will take the dangling free type variables in our type and create a
new type scheme that binds them by quantifying over them.
\[ \overline{\Gamma}(\alpha \rightarrow \alpha) = \forall \alpha . \alpha \rightarrow \alpha \]
And now that we have it quantified, \textsc{Var} is able to instantiate $y$
to \emph{both} an
${(\alpha \rightarrow \alpha) \rightarrow (\alpha \rightarrow \alpha)}$ and an ${\mathsf{Int} \rightarrow \mathsf{Int}}$.

\subsubsection{Syntax-Directed}
Note that we have been using the syntax-directed rules: a more modern
treatment of the original system. In the syntax-directed rules, the
typing relation assigns types to terms, not type
schemes~\cite[p.15]{tofte1988} as in~\cite{damas1982}. The original
system had six rules:
\begin{mathpar}
  \inferrule*[Right=Taut]{x : \sigma \in \Gamma}{\Gamma \vdash x : \sigma} \and
  \inferrule*[Right=Inst]{\Gamma \vdash e : \sigma \\ \sigma > \sigma'}{\Gamma \vdash e : \sigma'} \and
  \inferrule*[Right=Gen]{\Gamma \vdash e : \sigma \\ \alpha \notin \ftv(\Gamma)}{\Gamma \vdash e : \forall \alpha . \sigma}  \\
  \inferrule*[Right=Comb]{\Gamma \vdash e : \tau'\rightarrow\tau \\ \Gamma \vdash e' : \tau'}{\Gamma \vdash e \ e' : \tau}
  \and
  \inferrule*[Right=Abs]{\Gamma, x : \tau' \vdash e : \tau}{\Gamma \vdash \lambda x.e : \tau'\rightarrow\tau} \\
  \inferrule*[Right=Let]{\Gamma \vdash e : \sigma \\ \Gamma,x :\sigma \vdash e' : \tau}{\Gamma \vdash
    \letin{x=e}{e'} : \tau}
\end{mathpar}
As we can see, the syntax-directed rules have merged \textsc{Taut} and
\textsc{Inst} into \textsc{Var}, and \textsc{Gen} into \textsc{Let}
via $\overline{\Gamma}$. It is possible to prove that these two systems are
equivalent, but the benefit of having just four rules is that we now
have exactly one rule for each syntactic form of expression. This
means that \emph{the shape of the proof is identical to the shape of
  the syntax}. This is discussed in more detail in Section~\ref{sec:almost-synt-direct}.

\subsection{Dynamic Semantics}

You might have noticed that the previous section was titled
\textit{Static Semantics}. Whenever we created the
typing relations and the typing rules, we assigned a sense of meaning
to these types. For example, something of $\tau \rightarrow \tau$ is going to be
a function, because our \textsc{Abs} rule dictates so. These semantics
given to our program are known \emph{statically}. Without knowing what
any of our programs evaluate to, we can work out their types.

Usually want to be able to show that these static semantics are
\emph{sound}, which in essence means if something statically has a
type $\tau$, it actually evaluates to something of type
$\tau$\footnote{Type soundness is discussed in more detail in
  Section~\ref{sec:type-soundness}}. But to show this, we first need
to actually define how things are evaluated. This is called the
\emph{dynamic semantics} of a language.

There are multiple ways to define the dynamic semantics. In the
original work, Milner and Damas defined a \emph{denotational}
semantics for the language~\cite{milner1978,damas1984}, which
describes evaluation in terms of mathematical objects and functions
operating on them. For brevity, we will omit the details here, and
relegate them to Appendix~\ref{sec:denot-semant-hindl}. The
important part is that once the dynamic semantics are in place, it is
possible to define semantic entailment:
\[ \Gamma \vDash e : \tau \]
Which can be read as, \textsl{``In the context $\Gamma$, $e$ evaluates to a
  value in $\tau$''}. Semantic soundness then boils down to showing that
if $e$ has type $\tau$, $e$ does actually evaluate to a value in $\tau$.
\[ \Gamma \vdash e : \tau \rightarrow \Gamma \vDash e : \tau \]

%%% Local Variables:
%%% TeX-master: "report"
%%% TeX-engine: luatex
%%% TeX-command-extra-options: "-shell-escape"
%%% End:

% LocalWords:  instantiation denotational
