\chapter{Background}\label{chapter:background}

\section{Separation logic}\label{sec:separationologic}
Much of the original inspiration for this work came from separation
logic, and how it can be used to formalise properties of concurrent
programs. Separation logic~\cite{ohearn2019,reynolds2002} is an
extension to Hoare logic, a system for formalising properties about
imperative programs, and has been used to prove properties about
resource access in concurrent programs~\cite{ohearn2007}. It is
formalised through \emph{specifications}, which consist of some code
that alongside a \emph{precondition} a \emph{postcondition}.
\[
  {\color{gray} \{\textit{precondition}\}} \
  \textit{code} \
  {\color{gray} \{\textit{postcondition}\}}
\]
The preconditions and postconditions make assertions about what points
to what inside the \textit{heap}. For example, in the example that
O'Hearn gives, a cyclic list is constructed from two pointers.
\begin{gather*}
  {\color{gray} \{ x \mapsto 0 * y \mapsto 0 \} }\\
  [x] = y; \\
  [y] = x; \\
  {\color{gray} \{ x \mapsto y * y \mapsto x \} }
\end{gather*}
The code sets the value of $x$ is set to the location of $y$, and
vice-versa. Separation logic extends Hoare logic with the notion of
splitting the heap. In the example above, instead of asserting
$x \mapsto y \wedge y \mapsto x$, the separation conjunction $*$ is used, which says
``$x$ points to $y$ \textit{and separately} $y$ points to $x$''. The
heap must be able to split into disjoint subheaps, such that
$x \mapsto y$ holds in one and $y \mapsto x$ in the other. This means that
$x$ and $y$ cannot point to the same location in the heap.
These preconditions and postconditions can be derived from so-called
small axioms, which for an imperative language might contain an axiom
for storing to the heap:
\[ {\color{gray} \{x \mapsto -\}} [x] = v {\color{gray} \{x \mapsto v\}} \]
The \textit{frame rule} allows a specification to be extended by
adding extra predicates that do not mention any variables used by the
code.
\[
  \infer{{\color{gray} \{p\}} \ \emph{code} \ {\color{gray} \{q\}}}
  {{\color{gray} \{p * r\}} \ \emph{code} \ {\color{gray} \{ q * r \}}} \
  \text{\parbox{2in}{where no variable
    occurring free in r is modified by \emph{code}}}
\]
This rule ends up being quite significant, as its the reason why
separation logic allows for programs to be reasoned about locally ---
the predicates involving the heap are shrunk so that they only contain
variables that the code modifies.

There is also the \emph{concurrency rule} for reasoning about
concurrency, which gives the precondition and postcondition needed for
two programs that are run in parallel.
\[
  \infer{ {\color{gray} \{p_1\}} \ \emph{code}_1 \ {\color{gray} \{q_1\}}
    \\
    {\color{gray} \{p_2\}} \ \emph{code}_2 \ {\color{gray} \{q_2\}}}
  { {\color{gray} \{p_1 * p_2\}} \ \emph{code}_1 \ || \ \emph{code}_2 \
    {\color{gray} \{q_1 * q_2\}}}
\]
We will see later on in Chapter~\ref{cha:evaluation} how the system we
end up developing parallels with this. 

Krishnaswami built upon separation logic with a higher-order ML like
language\cite{krishnaswami2006} with types. It utilised monadic
binding for sequencing, allowing imperative computation within an
otherwise pure language. Unlike our system, this included separation
logic predicates directly within the language. We will not work with
predicates, and instead push all our guarantees into the type level.

\section{Monads categorically}
As the language we will develop in Chapter~\ref{chapter:system} is
based around monads, it is a good idea to take a brief look at what
exactly they are, and dip our toes into the cool waters of category
theory. There is an infamous definition\footnote{\textsc{James Iry} A Brief, Incomplete, and Mostly Wrong History of Programming Languages} of a monad that goes like:
\begin{quote}
  \textsl{A monad is just a monoid in the category of endofunctors,
    what's the problem?}
\end{quote}
Lets break this down: A \textit{category} is a collection of objects
(like a set), and morphisms (like functions between sets). In the same
way, Haskell has \textsf{Hask} --- the category of types. Its objects
are types such as \mintinline{haskell}{Bool} and
\mintinline{haskell}{Int}, and its morphisms are functions taking a
type and returning another.
\begin{minted}{haskell}
Bool, Int :: * -- Bool and Int are types
True :: Bool
42 :: Int
f : Bool -> Int -- a morphism between Bool and Int
\end{minted}
A \textit{functor} is a mapping between categories that
preserves the structure. That is, a functor maps objects and morphisms
from one category to objects and morphisms in another category.
In the same way, a \mintinline{haskell}{Functor} in Haskell maps a
type to another type, and its functions to other functions with
\mintinline{haskell}{fmap}. 
\begin{minted}{haskell}
Maybe :: * -> *
Maybe Bool :: * -- Maps Bool to Maybe Bool
fmap :: (a -> b) -> (f a -> f b) -- Maps (a -> b) to (f a -> f b)
\end{minted}
Functors can have mappings to other functors, and these are known as
\emph{natural transformations}.
An \textit{endofunctor} is a \textit{functor} from one category to the
same category. That is, it maps objects and morphisms from one
category, to objects and morphisms in the same category. But what does
that mean in Haskell? Well, since \mintinline{haskell}{Functor} maps
types to types, we are mapping from \textsf{Hask} to \textsf{Hask} ---
all \mintinline{haskell}{Functor}s in Haskell are endofunctors! They
exist as type constructors with kind \mintinline{haskell}{* -> *}.

A \textit{monad} is then defined as:
\begin{enumerate}
\item An endofunctor $T : X \rightarrow X$
\item With a natural transformation to flatten two monads together

${\mu : T(T(X)) \rightarrow T(X)}$
\item And another natural transformation $\eta : X \rightarrow T(X)$
\item That satisfy some properties (the monad laws)
\end{enumerate}

Or in Haskell,
\begin{minted}{haskell}
class Functor m => Monad m a where
  join :: m (m a) -> m a
  return :: a -> m a
\end{minted}
However as you might be aware, the actual definition in Haskell uses
bind \mintinline{haskell}{>>=}, not the mathematical definition of
\mintinline{haskell}{join}. It can be implemented in terms of
\mintinline{haskell}{>>=} instead, and vice-versa using the fact that
a monad is also a functor.
\begin{minted}{haskell}
(>>=) :: m a -> (a -> m b) -> m b
join m = m >>= \x -> x
m >>= f = join (fmap f m)
\end{minted}

The \textit{monoid in the category of endofunctors} bit then comes
about because a monoid is defined as:
\begin{enumerate}
\item A set $X$
\item With an associative binary operation $\circ : X \times X \rightarrow X$
\item And an identity element $e : X$
\item That satisfy some properties (the monoid laws)
\end{enumerate}

A monoid is the same deal as a monad, except functor composition is
replaced with the Cartesian product and the unit function is replaced
with the identity element.  And if monoids allow chaining together
elements, then monads in programming allow sequencing together
computation. In our system we will be using the more pragmatic Haskell
definition of a monad that uses bind rather than join. But the
fundamental idea of sequencing remains the same.

\section{Type-level programming within GHC}

\setminted[haskell]{breaklines}

Before creating the system in Chapter~\ref{chapter:system}, we
originally attempted to embed a resourceful system within GHC's type
system. Whilst the original Haskell 2010 language specification was
based on System F, there has been much
work~\cite{eisenberg2016}\cite{weirich2017} carried out to add
dependent types to GHC via a plethora of language extensions. Namely,
\texttt{TypeFamilies} and \texttt{GADT} provide a lot of the power
needed to carry out type-level programming, and we leveraged this
expressiveness to emulate a resource-tracking version of the
\mintinline{haskell}{IO} monad, dubbed \mintinline{haskell}{SubIO}. It
carried around a heap with a phantom type parameter:
\begin{minted}{haskell}
newtype SubIO (a :: [j]) b = SubIO (IO b)
  deriving (Functor, Applicative, Monad)
\end{minted}

We then defined a subclass of \mintinline{haskell}{Monad} that
extended it with a sequencing operator and concurrent operator, both
which merged the heap phantom type parameters together.
\begin{minted}{haskell}
class Monad (m j) => HeapMonad (m :: [x] -> * -> *) j where
  (>>>=) :: m j a -> (a -> m k b) -> m (j ** k) b
  (|||) :: m j a -> m k b -> m (j ** k) b
  (|||) x f = x >>>= const f
\end{minted}
The \texttt{**} operator was a type family that merged
together two heaps, and threw a type error if they overlapped.
\begin{minted}{haskell}
type family Overlap a b :: Bool where
  Overlap '[] b = 'False
  Overlap (a ': as) b = If (MemberP a b) 'True (Overlap as b)

type family a ** b :: [c] where
  (a ** b) = If (Overlap a b)
                (TypeError ('Text "Heaps overlap!"))
                (a :++ b)
\end{minted}
An instance for \mintinline{haskell}{SubIO} was given, and with it
some file operations from \mintinline{haskell}{System.IO}, transformed to accept
the file argument as a type argument, so it could be tracked in the heap.
\begin{minted}{haskell}
instance HeapMonad SubIO j where
  (SubIO x) >>>= f = SubIO (x >>= \z -> let (SubIO y) = f z in y)
  (SubIO x) ||| (SubIO y) = SubIO (forkIO (x >> pure ()) >> y)

readFile :: forall filePath. KnownSymbol filePath => SubIO '[filePath] String
readFile = fileOp Prelude.readFile

appendFile :: forall filePath. KnownSymbol filePath => String -> SubIO '[filePath] ()
appendFile x = fileOp (`Prelude.appendFile` x)

writeFile :: forall filePath. KnownSymbol filePath => String -> SubIO '[filePath] ()
writeFile x = fileOp (`Prelude.writeFile` x)

fileOp :: forall filePath a. KnownSymbol filePath => (FilePath -> IO a) -> SubIO '[filePath] a
fileOp f = let fp = symbolVal (Proxy :: Proxy filePath)
             in SubIO (f fp)
\end{minted}
All together, this meant that functions could be written with the
familiar do syntax, and ran concurrently with
\texttt{|||} --- but only if their heaps did not overlap.
\begin{minted}{haskell}
someIO = do
  writeFile @"foo.txt" "hello"
  s <- readFile
  return s

someMoreIO x = do
  appendFile @"bar.txt" x
  readFile >>= SubIO . putStrLn

concExample = runSubIO $ someIO ||| someMoreIO "blah"
--- this *should* type error
-- badConcExample = runSubIO $ someIO ||| someIO
\end{minted}
However we soon found out that overlapping instances don't always
throw a type error, as the phantom type parameter keeping track of the
heap was evaluated \textbf{lazily}. There was a workaround that
involved extracting the heap down to the term level and forcing
evaluation via \mintinline{haskell}{Data.Proxy} and
\mintinline{haskell}{seq}, but we think there are most likely better
ways to embed this within GHC's type system.

\section{Hindley-Damas-Milner}

One of our aims when developing this resource-tracking monadic type
system is to see how well it would integrate into existing functional
programming languages, specifically those of the ML family. So for
this reason we will basing our system off of the Hindley-Damas-Milner
(HDM) type system~\cite{damas1982}. Originally designed to formalise
the type system of ML, it is one of the first formalisations of a
polymorphic type system. It was heavily influential at the time and
still continues to be so today, paving the way for newer type systems
such as System F. 

\subsection{Syntax}

Before we can talk about a type system, we need to talk about the
language that it operates on. HDM uses an applicative language: A
language in which you can apply abstractions.
\def\defaultHypSeparation{\hskip .05in}
\newcommand{\letin}[2]{\mathsf{let} \ #1 \ \mathsf{in} \ #2}
\begin{grammar}

  <expression $e$> ::= $x$ | $\lambda x . e$ | $e \ e'$ | $\letin{x=e'}{e}$ % | $\square$

  <type $\tau$> ::= $\square$ | $\alpha$ | $\tau' \rightarrow \tau$
  
  <type scheme $\sigma$> ::= $\tau$ | $\forall \alpha . \sigma$

  <context $\Gamma$> ::= $\centerdot$ | $\Gamma, x : \sigma$

\end{grammar}
Expressions are an extension of the venerable lambda calculus, with the
addition of a new let expression that binds an expression to a
variable. As we will see later, this is notably different from
abstraction as it provides the gateway to polymorphism.

Types are either unit types, type variables or function types. However
expressions are not assigned types directly, instead they are given
type schemes which quantify over type variables. The distinction
between the two is necessary so that quantifiers can only
appear at the top level. 

A context is a linked list for looking up the type schemes of
variables. Whenever we need to work out the type of a variable
expression like $x$, we traverse the context to find its type scheme.

There is also a notion of \emph{free type variables}, which are type
variables \emph{inside a type scheme} which have not been bound
(quantified over).
\begin{align*}
  \ftv(\forall \alpha . \tau) &= \ftv(\tau) \setminus \{\alpha\} \\
  % \ftv(\square) &= \{ \} \\
  \ftv(\alpha) &= \{ \alpha \} \\
  \ftv(\tau \rightarrow \tau') &= \ftv(\tau') \cup \ftv(\tau)
\end{align*}

% A substitution $s$ maps type variables to types. A substitution of the
% form $[\tau/\alpha]$ maps $\alpha$ to $\tau$.

% It is extended to operate on types, such that every occurance of a
% type variable in a type is substituted.
% For instance, $(\alpha \rightarrow \alpha)[\tau/\alpha] = \tau \rightarrow \tau$ can be read as ``$(\alpha \rightarrow \alpha)$,
% replacing every $\alpha$ with $\tau$.''

% % \begin{minipage}{1.0\linewidth}
%   Substitution is \textbf{associative}. It is defined on types as
%   \begin{align*}
%     % \square [\tau/\alpha] &= \square \\
%     \alpha' [\tau/\alpha] &=
%                             \begin{cases}
%                               \tau & \mathsf{if} \ \alpha' = \alpha \\
%                               \alpha' & \mathsf{otherwise}
%                             \end{cases} \\
%     (\tau_1 \rightarrow \tau_2)[\tau/\alpha] &= (\tau_1[\tau/\alpha] \rightarrow \tau_2[\tau/\alpha])
%   \end{align*}
% % \end{minipage}

% And on type schemes as
% \[(\forall \alpha . \sigma)[\tau/\alpha] = \forall \tau . \sigma[\tau/\alpha]\]

% We may write $S(\sigma)$ to apply an arbitrary substitution to a type
% scheme.

% $[\tau_1/\alpha_1, \ldots, \tau_n/\alpha_n]$ may be used to notate the composition of
% substitutions $S_1 = [\tau_1/\alpha_1], \ldots, S_n = [\tau_n/\alpha_n]$, $S_1(\ldots(S_n(\sigma)))$.

% We say a type $\tau$ is an \textit{instance} of a type $\tau'$,
% written as $t > \tau'$ if there exists a substitution $S$ such that

% \[ \tau > \tau' \rightarrow \exists S. S(\tau) = \tau' \]

% Intuitively, this can be thought of as $\tau$ being more general than
% type $\tau'$. For example,
% \[ \alpha \rightarrow \beta > \alpha \rightarrow \alpha \]
% with the substitution $[\alpha/\beta]$, but there exists no substitution for
% \[ \alpha \rightarrow \alpha \ngtr \alpha \rightarrow \beta \]

\subsection{Static semantics}

A type system describes how we assign \emph{valid} types to our
expressions. Most of the time, it's through a typing relation like this
\[ \Gamma \vdash e : \tau \]
You can read this as ``\textit{In the context
  $\Gamma$, $e$ has the type $\tau$}''. This is a relation between a context,
an expression and a type, in the same way that $a \leq b$ is a relation
between two numbers. With relations we can form \textit{judgements},
such as ${1 \leq 2}$, or
${\Gamma \vdash \lambda x . (y \ x) : \alpha \rightarrow \tau}$.  These are just statements that we can
make --- they might be true, they might be false. Is $42 \leq 19$? Does
${\Gamma \vdash z : \beta}$? I don't know, you tell me. We need something else to be
able to tell whether or not they make sense: whether or not they are
\textit{well typed}. So there are \emph{typing rules} that allow us to
prove that these typing judgements are indeed well typed, and that the
expressions can have the type that they claim to have.
\begin{mathpar}
  \inferrule*[Right=Var]{x : \sigma \in \Gamma \\ \sigma > \tau}{\Gamma \vdash x : \tau} \and
  \inferrule*[Right=App]{\Gamma \vdash e : \tau' \rightarrow \tau \\ \Gamma \vdash e' : \tau'}{\Gamma \vdash e \ e' : \tau} \\
  \inferrule*[Right=Abs]{\Gamma,x:\tau' \vdash e : \tau}{\Gamma \vdash \lambda x . e : \tau' \rightarrow \tau} \and
  \inferrule*[Right=Let]{\Gamma \vdash e' : \tau' \\ \Gamma,x : \bar{\Gamma}(\tau') \vdash e : \tau}
  {\Gamma \vdash \mathsf{let} \ x = e' \ \mathsf{in} \ e : \tau}
\end{mathpar}
These rules consist of some \emph{premises} above a line, and a
\emph{conclusion} below it. Premises and conclusions are just other
judgements, that we have been able to prove.
The gist of these rules is that if you have proof of all the premises
above, then you can infer the conclusion at the bottom. So for example, you
can read \textsc{App} as \textsl{``If $e$ has the function type $\tau' \rightarrow \tau$
  in $\Gamma$ and $e'$ has the type $\tau'$ also in $\Gamma$, then $e'$ applied to
  $e$ has the type $\tau$ in $\Gamma$ too.''}.

\textsc{App} is one of the four typing rules in the syntax-directed
HDM type system, and tells us what happens to the types when we apply
an argument to a function. Let's take a look at what the others mean.
\textsc{Abs} relates to functions, sometimes called abstractions. It
says if $e$ has the type $\tau$ in the context $\Gamma$, \textit{extended}
with $x$ having the type scheme $\tau'$, then $\lambda x.e$ has the type ${\tau' \rightarrow
\tau}$ in $\Gamma$. In other words, if $e$ can have type $\tau$ provided it has
access to $x : \tau'$, then we can make a lambda out of it.

If we have a variable expression $x$, then \textsc{Var} tells us how
we can get the type for it. First, we need to make sure $x$ exists in
the context $\Gamma$. It will have some type scheme $\sigma$, but we can't
directly assign that to an expression --- the typing relation assigns
types to terms, not type schemes. Instead, we need to instantiate it
to a type with $\sigma > \tau$. This instantiation relation says that if we
have a type scheme $\forall \alpha_1\ldots\alpha_n . \tau'$, then there exists a mapping of
type variables to types
${\alpha_1\mapsto\tau_1,\ldots,\alpha_n\mapsto\tau_n}$ which we can apply to those bound type variables
in $\tau'$ to give us $\tau$.

As it turns out, these mappings from type
variables to types are really common, and they are called
\emph{substitutions}.  The whole rule then means, if we can look up a
type scheme $\sigma$ for $x$, and then instantiate to some type
$\tau$, then we can infer that $x$ has the type $\tau$ in $\Gamma$.

The \textsc{Let} rule introduces polymorphism to the language. That
is, it allows one expression to be used for multiple types. Consider
the identity function
\[ \lambda x . x \]
What type should it have? It could be used both as a
$\mathsf{Int} \rightarrow \mathsf{Int}$. But the reality is that we don't care
about the underlying type, and want it to work on \emph{anything}.
\[ \Gamma \vdash \lambda x . x : \alpha \rightarrow \alpha \] Now with type variables instead, we have
nicely abstracted over the concrete types. Assuming we have a context
like $\Gamma = \centerdot , a : \mathsf{Int}$, we might try to use it multiple
times, each with different types, by passing it in as an argument to
abstraction.
\[ \Gamma \vdash (\lambda y . (y \ y) \ (y \ a)) \ (\lambda x . x) : \mathsf{Int} \]
But this will not work, and it is in fact ill-typed. Our identity
function will get passed around as ${y : \alpha \rightarrow \alpha}$ through \textsc{Abs}. When
either $(y \ y)$ or $(y \ a)$ try to look up $y$ in \textsc{Var}, they
will find that they can't instantiate it to the type they want,
because $\alpha$ is not quantified over!

Instead, we need to pass around the identity function as ${y : \forall \alpha . \alpha
\rightarrow \alpha}$. With a let expression, we can instead say
\[ \Gamma \vdash \letin{y = \lambda x . x}{(y \ y) \ (y \ a)} : \mathsf{Int} \]
If we look at the first premise for \textsc{Let}, we can show that
${\Gamma \vdash \lambda x . x : \alpha \rightarrow \alpha}$. In the second premise however, we put it into
the context as $y : \overline{\Gamma}(\alpha \rightarrow \alpha)$. $\overline{\Gamma}(\alpha \rightarrow \alpha)$ is the
close function, which is defined as
\[ \overline{\Gamma}(\tau) = \forall \alpha_1 \ldots \alpha_n . \tau \
\textsf{where} \ \{ \alpha_1, \ldots, \alpha_n \} = \ftv(\tau) \setminus \ftv(\Gamma) \]
This will take the dangling free type variables in our type and create a
new type scheme that binds them by quantifying over them.
\[ \overline{\Gamma}(\alpha \rightarrow \alpha) = \forall \alpha . \alpha \rightarrow \alpha \]
And now that we have it quantified, \textsc{Var} is able to instantiate $y$
to \emph{both} an
${(\alpha \rightarrow \alpha) \rightarrow (\alpha \rightarrow \alpha)}$ and an ${\mathsf{Int} \rightarrow \mathsf{Int}}$.

\subsubsection{Syntax directed}
Note that we have been using the syntax-directed rules: a more modern
treatment of the original system. In the syntax-directed rules, the
typing relation assigns types to terms, not type
schemes~\cite[p.15]{tofte1988} as in~\cite{damas1982}. The original
system had six rules:
\begin{mathpar}
  \inferrule*[Right=Taut]{x : \sigma \in \Gamma}{\Gamma \vdash x : \sigma} \and
  \inferrule*[Right=Inst]{\Gamma \vdash e : \sigma \\ \sigma > \sigma'}{\Gamma \vdash e : \sigma'} \and
  \inferrule*[Right=Gen]{\Gamma \vdash e : \sigma \\ \alpha \notin \ftv(\Gamma)}{\Gamma \vdash e : \forall \alpha . \sigma}  \\
  \inferrule*[Right=Comb]{\Gamma \vdash e : \tau'\rightarrow\tau \\ \Gamma \vdash e' : \tau'}{\Gamma \vdash e \ e' : \tau}
  \and
  \inferrule*[Right=Abs]{\Gamma, x : \tau' \vdash e : \tau}{\Gamma \vdash \lambda x.e : \tau'\rightarrow\tau} \\
  \inferrule*[Right=Let]{\Gamma \vdash e : \sigma \\ \Gamma,x :\sigma \vdash e' : \tau}{\Gamma \vdash
    \letin{x=e}{e'} : \tau}
\end{mathpar}
As we can see, the syntax-directed rules have merged \textsc{Taut} and
\textsc{Inst} into \textsc{Var}, and \textsc{Gen} into \textsc{Let}
via $\overline{\Gamma}$. It is possible to prove that these two systems are
equivalent, but the benefit of having just four rules is that we now
have exactly one rule for each syntactic form of expression. This
means that \emph{the shape of the proof is identical to the shape of
  the syntax}. This is discussed in more detail in Section~\ref{sec:almost-synt-direct}.

\subsection{Dynamic Semantics}

Milner and Damas created a denotational semantics for the language.

The semantics is defined by a semantic algebra, which itself is
comprised of a \textit{semantic domain} and \textit{semantic
  equation}.

The semantic domain defines the possible values an expression in our
language can have. It is a \textbf{complete partial order} (often referred to
as a \textit{cpo}): A pair $(D, \sqsubseteq)$ of a set $D$ and a partial order
$\sqsubseteq$ (a function that orders elements in $D$, but not necessarily all
of them, hence the term \textit{partial}), such that:

\begin{enumerate}
\item there is a least element $\bot$
\item each directed subset $x_0 \sqsubseteq \ldots \sqsubseteq x_n \sqsubseteq \ldots$ has a least upper bound
  (lub)
\end{enumerate}

\begin{align*}
  \mathbb{V} &= \mathbb{B}_{\square} + \mathbb{B}_{bool} + \mathbb{F} + \mathbb{W} \\
  \mathbb{F} &= \mathbb{V} \rightarrow \mathbb{V} \\
  \mathbb{W} &= \{ . \}
\end{align*}

Or visually,

\begin{center}
  \tikz \graph[layered layout] { "$\mathbb{V}$" ->
    { "$\mathbb{B}_{\square}$" -> "$\square$",
      "$\mathbb{B}_{\textrm{Bool}}$" -> {true, false},
      I} ->
    "$\bot$"; };
\end{center}


Function space $D \rightarrow E$

Coalesced sum $D + E$

Since this cpo $\mathbb{V}$ represents all possibly data values, we
can extract a subset of it to model the values of certain
types %~\ref{shamirwadge77}.
A subset $I$ of our cpo $\mathbb{V}$ is called an ideal, iff it
satisfies the following properties:

\begin{enumerate}
\item it is downwards closed: $\forall v_0 \in V, v_1 \in V, v_0, v_0 \sqsubseteq v_1 \rightarrow
  v_0 \in I \rightarrow v_1 \in I$.
  
\item it is closed under lubs of \omega-chains.
\end{enumerate}

Our function domain $\mathbb{F}$ is a map from $\mathbb{V}$ to
$\mathbb{V}$. Maps over ideals are defined as
$I \rightarrow I' \equiv \{ v \in V | v \in \mathbb{F} \ \mathsf{and} \ \forall v' \in I \
(v_{|\mathbb{F}})v' \in I' \}$.

With all the mechanisms in place, we can now define what it means for
a value to semantically be a type:

\[v \in \mathbb{V}^\tau \iff \vDash v : \tau\]

Note that $v : \tau$ is a relation, not a function -- a value can be a
member of multiple types, and this should be read as ``$v$ is a $\tau$''.

\subsubsection{Bottom}
$\bot$ ends up being very useful to represent values that don't exist.
Take for example the following program which doesn't terminate. 

\[\letin{x = \lambda y. y}{x x}\]

What type does should this program have? It should assume the
type of whatever is needed. For instance, we would expect this program
to have a type of $\square$ as the argument is not used.

\[ (\lambda z. \square) (\letin{x = \lambda y. y}{x x}) : \square \]

Since $\bot$ is a member of all ideals, this program is well typed.

\subsubsection{Some notation}

If $\mathbb{D} \subset \mathbb{V}$, and $d \in \mathbb{D}$, we will say $d \ \mathsf{in} \
\mathbb{V}$ to represent $d$ but treated as if its in $\mathbb{V}$. \\
We will then define the reverse

$$v | \mathbb{D} =
\begin{cases}
  d & \textsf{if} \ v = d \ \textsf{in} \ \mathbb{V} \ \textsf{for
    some} \ d \in \mathbb{D} \\
  \bot_{\mathbb{D}} & \textsf{otherwise}
\end{cases}
$$


\subsubsection{Evaluation function}
The semantic equation $\mathcal{E} : \mathsf{Expression} \rightarrow
\mathsf{Environment} \rightarrow \mathbb{V}$ lies at the heart of the semantics,
and defines how the syntax is evaluated.

\begin{align*}
  \mathcal{E} \llbracket x \rrbracket \eta
  &= \eta \llbracket x \rrbracket \\
  \mathcal{E} \llbracket e_1 e_2 \rrbracket \eta
  &=
    \begin{cases}
      \bot & \mathsf{if} \ v_1 = \bot \\
      (v_1 | \mathbb{F}) v_2 & \mathsf{if} \ v_1 \in \mathbb{F} \\
      \mathsf{wrong} & \mathsf{otherwise}
    \end{cases}
  \\
  & \quad \textsf{where} \ v_i = \mathcal{E} \llbracket e_i \rrbracket \eta , \ i = \{
    1, 2\} \\
  \mathcal{E} \llbracket \lambda x . \ e \rrbracket \eta
  &=
    (\lambda v . \ \mathcal{E} \llbracket e \rrbracket \eta [v / x ])
    \ \mathsf{in} \ \mathbb{V} \\
  \mathcal{E} \llbracket \textsf{let} \ x = e_1 \ \textsf{in} \ e_2 \rrbracket \eta
  &=
    \mathcal{E} \llbracket e_2 \rrbracket \ \eta [ \mathcal{E} \llbracket e_1 \rrbracket\rho / x ]
\end{align*}

Note that these evaluation rules are not as strict as the semantics
defined by Milner~\cite{milner1978} -- namely, that the second argument
of application and let binding is not checked if it is a \textsf{wrong}.

We introduce the notion of an environment $\eta : \mathsf{Variable} \rightarrow
\mathbb{V}$. It is a map of variables bound to values.

An environment $\eta$ can be said to \textit{respect} a type environment
$\Gamma$ if all bindings in $\Gamma$ can be found in $\eta$ with the same type.
$$\eta : \Gamma \iff \forall x : \tau \in \Gamma. \ \eta \llbracket x \rrbracket : \tau$$

$$\Gamma \vDash e : \tau \iff
\forall \eta. \ \eta : \Gamma \rightarrow \mathcal{E} \llbracket e \rrbracket \eta : \tau $$

An assertion of the form above is said to be \textit{closed} if there
are no free type variables in $\Gamma$ or $\tau$, and an assertion only holds
iff its closed instances hold.

Let $\overline{\mathbb{V}}$ be the set of all ideals in $\mathbb{V}$
that do not contain $\mathsf{wrong}$.

There is also a type evaluation function $\mathcal{T} : \mathsf{Type}
\rightarrow \mathsf{Valuation} \rightarrow \overline{\mathbb{V}}$

\begin{align*}
  \mathcal{T}\llbracket \square \rrbracket\psi &= \mathbb{B}_{ \square } \\
  \mathcal{T}\llbracket \mathsf{Bool} \rrbracket \psi &= \mathbb{B}_{\mathsf{Bool}} \\
  \mathcal{T}\llbracket \alpha \rrbracket \psi &= \psi \llbracket \alpha \rrbracket \\
  \mathcal{T} \llbracket \tau \rightarrow \tau' \rrbracket \psi &= \mathcal{T}\llbracket \tau \rrbracket \psi \ \rightarrow \
                             \mathcal{T} \llbracket \tau' \rrbracket \psi
\end{align*}

\subsection{Correctness}

Both Milner~\cite{milner1978} and Damas~\cite{damas1982} proved semantic
soundness for the system.

TODO: Is this too trivial to be a lemma? The proof is pretty vacuous
\newtheorem{lemma}{Lemma}
\begin{lemma}[to be named]\label{lem:1}
  If $v : \tau$ and $\eta : \Gamma$, then $\eta[v/x] : \Gamma,x : \tau$
\end{lemma}
\begin{proof}
  If $\eta : \Gamma$, then $\forall x : \tau \in \Gamma \ \eta\llbracket x \rrbracket : \tau$.
  And if $v : \tau$ then $\eta[v/x] \llbracket x \rrbracket : \tau$.
  So $\eta[v/x] : \Gamma,x : \tau$, because for all $y : \tau' \in \Gamma,x : \tau$, either $y =
  x$ and the substitution evaluates to the right type, or $y \neq x$ but
  because $\eta : \Gamma$, we have $\eta \llbracket y \rrbracket : \tau'$.
\end{proof}

\newtheorem{theorem}{Theorem}
\begin{theorem}[Semantic Soundness]
  $\Gamma \vdash e : \tau \rightarrow \Gamma \vDash e : \tau$ \\
  If $e$ has type $\tau$, $e$ does actually evaluate to a value in $\tau$.
\end{theorem}
\begin{proof}
  We need to prove $\forall \eta. \eta : \Gamma \rightarrow \mathcal{E} \llbracket e \rrbracket \eta : \tau$. We can do
  this via induction on $e$:

  \begin{description}
  \item[\boxed{x}] This is the base case of the induction. Since
    $\Gamma \vdash x : \tau$ , the rule \textsc{Var} gives us
    $x : \tau \in \Gamma$. The evaluation function produces
    $\mathcal{E} \llbracket x \rrbracket \eta = \eta \llbracket x \rrbracket$, but because
    $\eta : \Gamma$, $x : \tau \in \eta$, so $\eta \llbracket x \rrbracket : \tau$.
  \item[\boxed{e_1 e_2}] The type given is
    $\Gamma \vdash e_1 e_2 : \tau$, and via \textsc{App} we have
    $\Gamma \vdash e_1 : \tau' \rightarrow \tau$ and
    $\Gamma \vdash e_2 : \tau'$.  By applying the induction hypothesis, we get
    $\Gamma \vDash e_1 : \tau' \rightarrow \tau$ and $\Gamma \vDash e_2 : \tau'$, so
    $v_1 \in \tau' \rightarrow \tau$ and therefore $v_1 \in \mathbb{F}$. \\
    This brings us to
    $\mathcal{E} \llbracket e_1 e_2 \rrbracket \eta = (v_1 | \mathbb{F}) v_2$. We can tell
    what the application of the $v_2$ will give us by looking at the
    definition for the ideal of $v_1$:
    ${\tau' \rightarrow \tau = \{ v \in \mathbb{V} | v \in \mathbb{F} \wedge \forall v' \in \tau' (v |
      \mathbb{F}) v' \in \tau \}}$.  So
    $(v_1 | \mathbb{F}) v_2 \in \tau$, as is required to show
    $\Gamma \vDash e_1 e_2 : \tau$.
  \item[\boxed{\lambda x . e}] Our type is
    $\Gamma \vdash \lambda x . e : \tau' \rightarrow \tau$ and our typing rule \textsc{Abs} tells us
    the antecedent is $\Gamma,x:\tau' \vdash e :
    \tau$. Evaluating our expression gives
    $\mathcal{E} \llbracket \lambda x . e \rrbracket \eta = (\lambda v. \mathcal{E} \llbracket e \rrbracket \eta[v / x]) \
    in \ \mathbb{V}$. \\
    If we can prove
    $\mathcal{E} \llbracket e \rrbracket \eta [v/x] : \tau$ where
    $v : \tau'$, then we can prove that
    ${\lambda v . \mathcal{E} \llbracket e \rrbracket \eta [v/x] : \tau' \rightarrow \tau}$. But note that for
    $\Gamma, x : \tau' \vdash e : \tau$, lemma~\ref{lem:1} tells us $\eta[v/x] : \Gamma,x : \tau'$.
    And with this fact, $\Gamma,x : \tau' \vDash e : \tau$ from the
    induction hypothesis gives us $\mathcal{E} \llbracket e \rrbracket \eta [v/x] : \tau$
    where $v : \tau'$, as needed.
  \item[\boxed{\letin{x = e_1}{e_2}}] $\Gamma \vdash \letin{x = e_1}{e_2} : \tau$, and
    \textsc{Let} tells us $\Gamma \vdash e_1 : \tau'$ and $\Gamma,x : \tau' \vdash e_2 : \tau$. The
    evaluation function for let expressions is
    ${\mathcal{E} \llbracket \letin{x = e_1}{e_2} \rrbracket \eta
    = \mathcal{E} \llbracket e_2 \rrbracket (\eta [\mathcal{E} \llbracket e_1 \rrbracket \eta / x ])}$. We know that
  $\mathcal{E} \llbracket e_1 \rrbracket \eta : \tau'$ because of the induction hypothesis on
  $\Gamma \vdash e_1 : \tau'$, and will refer to this value as $v_1 : \tau'$. \\
  From lemma~\ref{lem:1}, in $\Gamma,x : \tau' \vdash e_2 : \tau$ the substituted environment
  respects the type environment $\eta[v_1/x] : \Gamma,x : \tau'$.
  Therefore $\Gamma,x : \tau' \vDash e_2 : \tau$ tells us that
  $\mathcal{E} \llbracket e_2 \rrbracket (\eta [v_1/x]) : \tau$. This is
  identical to our hypothesis, and our proof is done.
  \end{description}
  
\end{proof}

%%% Local Variables:
%%% TeX-master: "report"
%%% TeX-engine: luatex
%%% TeX-command-extra-options: "-shell-escape"
%%% End:

% LocalWords:  instantiation
